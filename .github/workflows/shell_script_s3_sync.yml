name: Upload Shell Scripts to S3
permissions:
  contents: read

on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - 'shell_scripts/public_s3_bucket/**'

concurrency:
  group: s3_sync_process_shell_scripts
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  upload:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    steps:
      - name: Checkout gh_hdc
        uses: actions/checkout@v6
        with:
          repository: mike12806/gh_hdc
          token: ${{ secrets.GH_HDC_PAT }}
          path: gh_hdc

      - name: Connect to Tailscale
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:ci
          hostname: github-runner-shell-script-s3
          version: latest

      - name: Cache AWS CLI
        uses: actions/cache@v4
        with:
          path: |
            /usr/local/aws-cli
            /usr/local/bin/aws
          key: ${{ runner.os }}-awscli-v2
          restore-keys: |
            ${{ runner.os }}-awscli-

      - name: Install dependencies
        run: |
          sudo apt-get update -y
          sudo apt-get install -y curl unzip
          curl -fsSL "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          sudo /tmp/aws/install
          rm -rf /tmp/awscliv2.zip /tmp/aws

      - name: Configure AWS CLI
        run: |
          sudo aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          sudo aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          sudo aws configure set default.region ${{ secrets.AWS_REGION }}
          sudo aws configure set s3.endpoint_url ${{ secrets.AWS_ENDPOINT_URL }}

      - name: Download files from S3
        run: |
          sudo mkdir -p s3_files
          sudo chown -R $USER:$USER s3_files  # Ensure correct ownership
          sudo chmod -R 755 s3_files  # Ensure the directory is writable
          sudo aws s3 sync s3://public-scripts/ s3_files/ --endpoint-url=${{ secrets.AWS_ENDPOINT_URL }}

      - name: Compare files
        run: |
          sudo mkdir -p changed_files
          sudo chown -R $USER:$USER changed_files  # Ensure correct ownership
          sudo chmod -R 755 changed_files  # Ensure the directory is writable
          
          # Compare files and copy changed files to the changed_files directory
          for file in gh_hdc/shell_scripts/public_s3_bucket/**; do
            if [ -f "$file" ]; then
              # Check if the file exists in S3 or is different
              if ! diff -q "$file" "s3_files/$(basename $file)" > /dev/null 2>&1; then
                sudo cp "$file" "changed_files/"
              fi
            fi
        run: |
          shopt -s globstar

      - name: Upload changed files to S3
        run: |
          if [ "$(ls -A changed_files)" ]; then
            for file in changed_files/**; do
              echo "Uploading $file to S3"
              sudo aws s3 cp "$file" "s3://public-scripts/$(basename $file)" --endpoint-url=${{ secrets.AWS_ENDPOINT_URL }}
            done
          else
            echo "No files to upload."
          fi

      - name: Delete files from S3 that are no longer in Git
        run: |
          # Get a list of files that exist in the local directory
          LOCAL_FILES=$(find gh_hdc/shell_scripts/public_s3_bucket -type f -exec basename {} \;)
          
          # List files in the S3 bucket
          S3_FILES=$(sudo aws s3 ls s3://public-scripts/ --recursive --endpoint-url=${{ secrets.AWS_ENDPOINT_URL }} | awk '{print $4}')
          
          # Check for files in S3 that are not in the local directory and delete them
          for s3_file in $S3_FILES; do
            if ! echo "$LOCAL_FILES" | grep -q "$(basename "$s3_file")"; then
              echo "Deleting $s3_file from S3"
              sudo aws s3 rm "s3://public-scripts/$s3_file" --endpoint-url=${{ secrets.AWS_ENDPOINT_URL }}
            fi
          done

      - name: Cleanup
        if: always()
        run: |
          sudo rm -rf s3_files
          sudo rm -rf changed_files
